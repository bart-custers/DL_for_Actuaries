{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the French MTPL Claims Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset (replace 'path_to_dataset.csv' with your actual file path)\n",
    "data = pd.read_csv('path_to_dataset.csv')\n",
    "\n",
    "# Inspect the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Step 2: Preprocess the Dataset\n",
    "# Assume dataset has numerical and categorical features\n",
    "numerical_features = ['Exposure', 'VehPower', 'VehAge', 'DrivAge']\n",
    "categorical_features = ['Area', 'VehBrand', 'VehGas']\n",
    "\n",
    "y = data['ClaimAmount']\n",
    "X = data.drop('ClaimAmount', axis=1)\n",
    "\n",
    "# Create a preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)])\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply the preprocessing pipeline\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# Step 3: Fit a Generalized Linear Model (GLM)\n",
    "glm = PoissonRegressor()\n",
    "glm.fit(X_train, y_train)\n",
    "\n",
    "# Get GLM predictions\n",
    "glm_preds_train = glm.predict(X_train)\n",
    "glm_preds_test = glm.predict(X_test)\n",
    "\n",
    "# Evaluate GLM\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model_name, y_true, y_pred):\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{model_name} - RMSE: {rmse:.4f}, RÂ²: {r2:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.3)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], color='red', linewidth=2)\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.title(f'{model_name} Predictions vs. True Values')\n",
    "    plt.show()\n",
    "\n",
    "evaluate_model(\"GLM\", y_test, glm_preds_test)\n",
    "\n",
    "# Add GLM predictions as features to the dataset\n",
    "X_train = np.hstack((X_train.toarray(), glm_preds_train.reshape(-1, 1)))\n",
    "X_test = np.hstack((X_test.toarray(), glm_preds_test.reshape(-1, 1)))\n",
    "\n",
    "# Step 4: Build the Combined Actuarial Neural Network (CANN)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the CANN model\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "cann_model = models.Sequential([\n",
    "    layers.Input(shape=(input_shape,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "cann_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the CANN model\n",
    "cann_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate CANN\n",
    "cann_preds_test = cann_model.predict(X_test)\n",
    "evaluate_model(\"CANN\", y_test, cann_preds_test)\n",
    "\n",
    "# Step 5: Build the LocalGLMNet Model\n",
    "from torch import nn\n",
    "import torch\n",
    "from skorch import NeuralNetRegressor\n",
    "from torch.optim import NAdam\n",
    "\n",
    "class LocalGLMNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_sizes):\n",
    "        super(LocalGLMNet, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(input_size, hidden_layer_sizes[0])]\n",
    "        )\n",
    "        self.hidden_layers.extend(\n",
    "            [\n",
    "                nn.Linear(hidden_layer_sizes[i], hidden_layer_sizes[i + 1])\n",
    "                for i in range(len(hidden_layer_sizes) - 1)\n",
    "            ]\n",
    "        )\n",
    "        self.last_hidden_layer = nn.Linear(hidden_layer_sizes[-1], input_size)\n",
    "        self.output_layer = nn.Linear(1, 1)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.inverse_link = torch.exp\n",
    "\n",
    "    def forward(self, features, exposure=None, attentions=False):\n",
    "        x = features\n",
    "        for layer in self.hidden_layers:\n",
    "            x = self.activation(layer(x))\n",
    "        x = self.last_hidden_layer(x)\n",
    "        if attentions:\n",
    "            return x\n",
    "        skip_connection = torch.einsum(\"ij,ij->i\", x, features).unsqueeze(1)\n",
    "        x = self.output_layer(skip_connection)\n",
    "        x = self.inverse_link(x)\n",
    "        if exposure is None:\n",
    "            exposure = torch.ones_like(x, device=features.device)\n",
    "        x = x * exposure\n",
    "        return x\n",
    "\n",
    "# Step 6: Train the LocalGLMNet Model\n",
    "def train_model(X, v, y, device):\n",
    "    localglmnet = NeuralNetRegressor(\n",
    "        module=LocalGLMNet,\n",
    "        max_epochs=10,\n",
    "        criterion=nn.PoissonNLLLoss,\n",
    "        criterion__log_input=False,\n",
    "        module__input_size=X.shape[1],\n",
    "        module__hidden_layer_sizes=[64, 32, 16],\n",
    "        optimizer=NAdam,\n",
    "        lr=0.01,\n",
    "        batch_size=512,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    X_dict = {\"features\": X, \"exposure\": v}\n",
    "    localglmnet.fit(X_dict, y)\n",
    "    return localglmnet\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_torch = torch.FloatTensor(X_train)\n",
    "y_train_torch = torch.FloatTensor(y_train.values)\n",
    "X_test_torch = torch.FloatTensor(X_test)\n",
    "y_test_torch = torch.FloatTensor(y_test.values)\n",
    "\n",
    "# Example call to train the LocalGLMNet\n",
    "trained_model = train_model(X_train_torch, np.ones_like(y_train_torch), y_train_torch, device='cpu')\n",
    "\n",
    "# Evaluate LocalGLMNet\n",
    "localglmnet_preds_test = trained_model.predict(X_test_torch).detach().numpy()\n",
    "evaluate_model(\"LocalGLMNet\", y_test, localglmnet_preds_test)\n",
    "\n",
    "# Step 7: Build the TabNet Model\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "# Initialize and train the TabNet model\n",
    "tabnet = TabNetRegressor()\n",
    "tabnet.fit(X_train, y_train.values,\n",
    "           eval_set=[(X_test, y_test.values)],\n",
    "           eval_metric=['rmse'],\n",
    "           max_epochs=50)\n",
    "\n",
    "# Evaluate TabNet\n",
    "tabnet_preds_test = tabnet.predict(X_test)\n",
    "evaluate_model(\"TabNet\", y_test, tabnet_preds_test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
